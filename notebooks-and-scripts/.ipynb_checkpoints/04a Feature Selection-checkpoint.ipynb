{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.metrics import f1_score, confusion_matrix, mutual_info_score, roc_auc_score\n",
    "from helpers.machine_learning import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, chi2, RFECV, RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Overview\n",
    "In this section, we test a number of ideas regarding which features to transform into new features as well as which features to exclude. While it is possible to check how each individual modification of the feature set impacts the performance of various classification algorithms, it may also be the case that unintuitive combinations of modifications generate better results. Furthermore, we use for the last step in the feature selection process a [recursive feature elimination algorithm](http://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py) (RFE), which will attempt to select an optimal subset of features from an available feature set. Because algorithms such as RFE can be unstable and sensitive to small changes to the feature set it is selecting from, we run all combinations of feature modifications through RFE while testing their effect on a set of classification techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>year</th>\n",
       "      <th>age</th>\n",
       "      <th>congressional_district</th>\n",
       "      <th>state</th>\n",
       "      <th>gender</th>\n",
       "      <th>weight</th>\n",
       "      <th>final_vote</th>\n",
       "      <th>VCF0108</th>\n",
       "      <th>VCF0113</th>\n",
       "      <th>VCF0127</th>\n",
       "      <th>VCF0143</th>\n",
       "      <th>VCF0146</th>\n",
       "      <th>VCF0311</th>\n",
       "      <th>VCF0346</th>\n",
       "      <th>VCF0347</th>\n",
       "      <th>VCF0348</th>\n",
       "      <th>VCF0349</th>\n",
       "      <th>VCF0358</th>\n",
       "      <th>VCF0359</th>\n",
       "      <th>VCF0360</th>\n",
       "      <th>VCF0361</th>\n",
       "      <th>VCF0370</th>\n",
       "      <th>VCF0371</th>\n",
       "      <th>VCF0372</th>\n",
       "      <th>...</th>\n",
       "      <th>VCF0904_oh2</th>\n",
       "      <th>VCF0904_oh3</th>\n",
       "      <th>VCF1004_oh0</th>\n",
       "      <th>VCF1004_oh1</th>\n",
       "      <th>VCF1004_oh2</th>\n",
       "      <th>VCF1004_oh3</th>\n",
       "      <th>VCF1004_oh4</th>\n",
       "      <th>VCF9030_oh0</th>\n",
       "      <th>VCF9030_oh1</th>\n",
       "      <th>VCF9030_oh2</th>\n",
       "      <th>VCF9030_oh3</th>\n",
       "      <th>VCF9030_oh4</th>\n",
       "      <th>VCF9030_oh5</th>\n",
       "      <th>VCF9131_oh0</th>\n",
       "      <th>VCF9131_oh1</th>\n",
       "      <th>VCF9131_oh2</th>\n",
       "      <th>VCF9131_oh3</th>\n",
       "      <th>VCF9132_oh0</th>\n",
       "      <th>VCF9132_oh1</th>\n",
       "      <th>VCF9132_oh2</th>\n",
       "      <th>VCF9132_oh3</th>\n",
       "      <th>VCF9133_oh0</th>\n",
       "      <th>VCF9133_oh1</th>\n",
       "      <th>VCF9133_oh2</th>\n",
       "      <th>VCF9133_oh3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>49.0</td>\n",
       "      <td>MN01</td>\n",
       "      <td>MN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2886</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>35.0</td>\n",
       "      <td>MI01</td>\n",
       "      <td>MI</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8959</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>57.0</td>\n",
       "      <td>IL11</td>\n",
       "      <td>IL</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0454</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>63.0</td>\n",
       "      <td>ME02</td>\n",
       "      <td>ME</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6005</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>40.0</td>\n",
       "      <td>MA01</td>\n",
       "      <td>MA</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9270</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 403 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  year   age congressional_district state  gender  weight  \\\n",
       "0           0  2000  49.0                   MN01    MN       0  1.2886   \n",
       "1           1  2000  35.0                   MI01    MI       1  0.8959   \n",
       "2           2  2000  57.0                   IL11    IL       1  1.0454   \n",
       "3           3  2000  63.0                   ME02    ME       0  0.6005   \n",
       "4           4  2000  40.0                   MA01    MA       1  1.9270   \n",
       "\n",
       "   final_vote  VCF0108  VCF0113  VCF0127  VCF0143  VCF0146  VCF0311  VCF0346  \\\n",
       "0           2      0.0        0        0      1.0      1.0      1.0      1.0   \n",
       "1           0      0.0        0        0      1.0      1.0      0.0      1.0   \n",
       "2           1      0.0        0        0      1.0      1.0      1.0      0.0   \n",
       "3           1      0.0        0        1      0.0      1.0      1.0      0.0   \n",
       "4           2      0.0        0        0      1.0      1.0      1.0      1.0   \n",
       "\n",
       "   VCF0347  VCF0348  VCF0349  VCF0358  VCF0359  VCF0360  VCF0361  VCF0370  \\\n",
       "0      1.0      0.0      0.0      0.0      1.0      0.0      1.0      1.0   \n",
       "1      0.0      0.0      0.0      1.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      1.0      1.0      0.0      0.0      1.0      1.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      1.0      1.0      0.0      1.0      1.0      1.0      0.0   \n",
       "\n",
       "   VCF0371  VCF0372     ...       VCF0904_oh2  VCF0904_oh3  VCF1004_oh0  \\\n",
       "0      0.0      1.0     ...               1.0          0.0          0.0   \n",
       "1      0.0      0.0     ...               1.0          0.0          0.0   \n",
       "2      1.0      1.0     ...               1.0          0.0          0.0   \n",
       "3      0.0      1.0     ...               1.0          0.0          0.0   \n",
       "4      0.0      1.0     ...               1.0          0.0          0.0   \n",
       "\n",
       "   VCF1004_oh1  VCF1004_oh2  VCF1004_oh3  VCF1004_oh4  VCF9030_oh0  \\\n",
       "0          0.0          1.0          0.0          0.0          0.0   \n",
       "1          1.0          0.0          0.0          0.0          0.0   \n",
       "2          0.0          0.0          1.0          0.0          0.0   \n",
       "3          1.0          0.0          0.0          0.0          0.0   \n",
       "4          0.0          0.0          0.0          1.0          1.0   \n",
       "\n",
       "   VCF9030_oh1  VCF9030_oh2  VCF9030_oh3  VCF9030_oh4  VCF9030_oh5  \\\n",
       "0          1.0          0.0          0.0          0.0          0.0   \n",
       "1          0.0          0.0          0.0          0.0          1.0   \n",
       "2          0.0          0.0          0.0          0.0          1.0   \n",
       "3          0.0          1.0          0.0          0.0          0.0   \n",
       "4          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   VCF9131_oh0  VCF9131_oh1  VCF9131_oh2  VCF9131_oh3  VCF9132_oh0  \\\n",
       "0          0.0          1.0          0.0          0.0          0.0   \n",
       "1          0.0          0.0          1.0          0.0          0.0   \n",
       "2          0.0          0.0          1.0          0.0          0.0   \n",
       "3          0.0          1.0          0.0          0.0          0.0   \n",
       "4          0.0          1.0          0.0          0.0          0.0   \n",
       "\n",
       "   VCF9132_oh1  VCF9132_oh2  VCF9132_oh3  VCF9133_oh0  VCF9133_oh1  \\\n",
       "0          0.0          1.0          0.0          0.0          1.0   \n",
       "1          0.0          1.0          0.0          0.0          1.0   \n",
       "2          1.0          0.0          0.0          0.0          0.0   \n",
       "3          1.0          0.0          0.0          0.0          0.0   \n",
       "4          1.0          0.0          0.0          0.0          1.0   \n",
       "\n",
       "   VCF9133_oh2  VCF9133_oh3  \n",
       "0          0.0          0.0  \n",
       "1          0.0          0.0  \n",
       "2          1.0          0.0  \n",
       "3          1.0          0.0  \n",
       "4          0.0          0.0  \n",
       "\n",
       "[5 rows x 403 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig = pd.read_csv('../data/anes_cdf_converted.csv')\n",
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Testing classification models\n",
    "As testing will be conducted frequently, a simple testing function is constructed for convenience and flexibility. It outputs a table of results for each classification algorithm that is specified using a user-selected performance metric. **A note on computational complexity**: this testing method defaults to 5-fold cross-validation, which when applied to multiple classification algorithms can be time-consuming (15 to 30 seconds on a consumer-grade laptop). But because the dataset in question, at around 4000 training examples and 400 features, is not prohibitively large, and the classification task is not required to occur in real time, we consider the increased complexity of using 5-fold cross-validation an acceptable cost for the benefits of a more stable performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def cv_test(X_train, y_train, preprocessing, classifiers, clf_names, scoring = 'f1', cv = 5):\n",
    "    scores = []\n",
    "    if preprocessing != None:\n",
    "        X_cv = preprocessing.fit_transform(X_train)\n",
    "    else:\n",
    "        X_cv = X_train\n",
    "    for model, name in zip(classifiers, clf_names):\n",
    "        cv_score = cross_val_score(X = X_cv, y = y_train, estimator = model, cv = cv, scoring = scoring)\n",
    "        scores.append([cv_score.mean(), cv_score.ptp(), cv_score.std()])\n",
    "    scores = pd.DataFrame(scores, columns = ['mean','range','std'])\n",
    "    scores.index = clf_names\n",
    "    return scores.sort_values(by = 'mean', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Feature set modification ideas\n",
    "We consider five ideas for feature set modification:\n",
    "1. **Sentiment intensity from thermometer features**: From the collection of \"thermometer\" and \"index\" features, which measure the respondent's negative/positive sentiment towards a subject on a 0-100 scale, generate a set of sentiment intensity features that measure the distance of a response from neutral (a score of 50).\n",
    "2. **Response intensity from ordinal features**: Like the \"thermometer\" and \"index\" features mentioned above, a select set of ordinal features can also be converted in a similar fashion to measure intensity.\n",
    "3. **Number of times respondent answers \"don't know\"**: A number of interview questions offer the respondent the chance to answer \"don't know,\" and the proportion of non-voters tends to be significantly higher for the \"don't know\" categories of many questions.\n",
    "4. **Removing the first feature of a one-hot feature group**: Many categorical features were encoded as a collection of binary features in a process called one-hot encoding. If a categorical feature has $n$ response categories, then only $n-1$ binary features are required for encoding. Therefore, for each group of one-hot encoded features, we drop the first binary feature of the group.\n",
    "5. **Breaking correlation chains**: It is often the case that highly correlated features contain redundant data, but worse still is the fact that such correlations have the potential to render interpretations of feature importance in linear models meaningless. Thus, a number of redundant, collinear features are excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Setting a baseline performance score\n",
    "We must first find a baseline of performance with which to measure the results of modifications to the feature set against. Because, as mentioned before, the data is rather unbalanced with far more voters than non-voters, a performance metric such as accuracy becomes significantly less meaningful. For example, if 80% of respondents are voters, then a model that predicts every respondent is a voter has an accuracy of 80% despite being decidedly useless. Therefore, we use the [F1-score](https://en.wikipedia.org/wiki/F1_score), defined as the harmonic mean of [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall).\n",
    "\n",
    "### Addressing data leakage\n",
    "Prior to any baseline testing, we remove a number of features that would have resulted in [data leakage](https://www.kaggle.com/wiki/Leakage). Features like \"Who did respondent vote for president?\" or \"Did respondent register and vote?\" essentially give away whether or not the respondent voted.\n",
    "\n",
    "For the purposes of performance assessment, we also negate the target feature so that non-voters comprise positive cases while voters comprise negative cases. Because the data is unbalanced with almost 80% voters, treating voters as positive cases would have resulted in inflated F1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df_orig = df_orig.drop(['Unnamed: 0', 'congressional_district','state','final_vote'], axis = 1)\n",
    "\n",
    "df_orig = df_orig.iloc[:, ~df_orig.columns.str.contains('VCF0734')]\n",
    "df_orig = df_orig.iloc[:, ~df_orig.columns.str.contains('VCF0736')]\n",
    "df_orig = df_orig.iloc[:, ~df_orig.columns.str.contains('VCF1011')]\n",
    "df_orig = df_orig.iloc[:, ~df_orig.columns.str.contains('VCF0704')]\n",
    "df_orig = df_orig.iloc[:, ~df_orig.columns.str.contains('VCF0710')]\n",
    "df_orig = df_orig.iloc[:, ~df_orig.columns.str.contains('VCF0709')]\n",
    "df_orig = df_orig.iloc[:, ~df_orig.columns.str.contains('VCF0703')]\n",
    "df_orig = df_orig.iloc[:, ~df_orig.columns.str.contains('VCF0707')]\n",
    "df_orig = df_orig.iloc[:, ~df_orig.columns.str.contains('VCF0708')]\n",
    "df_orig = df_orig.iloc[:, ~df_orig.columns.str.contains('VCF1011')]\n",
    "\n",
    "# Label non-voters as positive cases and voters as negative cases\n",
    "df_orig.VCF0702 = df_orig.VCF0702.apply(lambda x: 0 if x==1 else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, we separate training (pre-2012) and test (2012) data so that we can test the generalizability of a model trained on data from earlier years to that of a target year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df = df_orig[:]\n",
    "df_train = df[df.year < 2012]\n",
    "\n",
    "todrop = ['VCF0702','year']\n",
    "X_train_orig = df_train.drop(todrop, axis = 1)\n",
    "y_train = df_train.VCF0702\n",
    "\n",
    "columns = X_train_orig.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "To address missing values in the data, median imputation is used to replace each feature's missing data with the median value of that feature. Its two main advantages over mean imputation is that it is less sensitive to the skewness of a feature's distribution and does not risk inadvertantly creating a third category in the case of binary features.\n",
    "\n",
    "All non-binary features are also normalized so that:\n",
    "1. A support vector machine, which is sensitive to feature scaling, can be included in the model testing\n",
    "2. A linear model like logistic regression will converge faster\n",
    "3. Principal component analysis can be used later for dimensionality reduction\n",
    "\n",
    "The StandardScaler provided by Scikit-learn performs scaling on all features and provides no means of excluding binary features from scaling. Thus, we build a custom transformer, Normalizer, to integrate into the preprocessing pipeline.\n",
    "\n",
    "We test six classification algorithms:\n",
    "- [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "- [Random Forest](http://blog.echen.me/2011/03/14/laymans-introduction-to-random-forests/)\n",
    "- [Adaptive Boosting](https://en.wikipedia.org/wiki/AdaBoost)\n",
    "- [Bernoulli Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Bernoulli_naive_Bayes)\n",
    "- [Gaussian Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Gaussian_naive_Bayes)\n",
    "- [Support Vector Machine](http://docs.opencv.org/2.4/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>range</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.629329</td>\n",
       "      <td>0.067231</td>\n",
       "      <td>0.024939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.625409</td>\n",
       "      <td>0.080189</td>\n",
       "      <td>0.031325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernoulli Naive Bayes</th>\n",
       "      <td>0.613127</td>\n",
       "      <td>0.074242</td>\n",
       "      <td>0.027320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.574183</td>\n",
       "      <td>0.083677</td>\n",
       "      <td>0.033105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes</th>\n",
       "      <td>0.550409</td>\n",
       "      <td>0.189112</td>\n",
       "      <td>0.071984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.523550</td>\n",
       "      <td>0.085928</td>\n",
       "      <td>0.031940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           mean     range       std\n",
       "AdaBoost               0.629329  0.067231  0.024939\n",
       "Logistic Regression    0.625409  0.080189  0.031325\n",
       "Bernoulli Naive Bayes  0.613127  0.074242  0.027320\n",
       "SVM                    0.574183  0.083677  0.033105\n",
       "Gaussian Naive Bayes   0.550409  0.189112  0.071984\n",
       "Random Forest          0.523550  0.085928  0.031940"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp = Imputer(missing_values = 'NaN', strategy = 'median')\n",
    "\n",
    "preprocessing = Pipeline([('imp',imp),('scale', Normalizer())])\n",
    "\n",
    "classifiers = [LogisticRegression(), RandomForestClassifier(), AdaBoostClassifier(),\n",
    "               BernoulliNB(), GaussianNB(), SVC()]\n",
    "clf_names = ['Logistic Regression','Random Forest', 'AdaBoost',\n",
    "             'Bernoulli Naive Bayes','Gaussian Naive Bayes', 'SVM']\n",
    "\n",
    "scores = cv_test(X_train_orig, y_train, preprocessing, classifiers, clf_names)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The results above show that Logistic Regression, AdaBoost, and Bernoulli Naive Bayes perform best on the training data while remaining reasonably stable, and Random Forest struggles the most. The performance of Gaussian Naive Bayes is average relative to the other models, though its cross-validation scores have the highest variability, indicating it is the least stable model.\n",
    "\n",
    "Because the dataset has many more features than is ideal for its size (just 3886 training examples for 361 features), some methods could be prone to overfitting the data. To see if any methods might perform better with a lower-dimensional feature space, we apply a modest degree of dimensionality reduction using PCA, projecting 361 features onto 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>range</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.636986</td>\n",
       "      <td>0.052743</td>\n",
       "      <td>0.019505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.582541</td>\n",
       "      <td>0.094152</td>\n",
       "      <td>0.033499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes</th>\n",
       "      <td>0.581306</td>\n",
       "      <td>0.045122</td>\n",
       "      <td>0.017064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.541544</td>\n",
       "      <td>0.119302</td>\n",
       "      <td>0.041342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernoulli Naive Bayes</th>\n",
       "      <td>0.461730</td>\n",
       "      <td>0.098944</td>\n",
       "      <td>0.040726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.286704</td>\n",
       "      <td>0.184333</td>\n",
       "      <td>0.063496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           mean     range       std\n",
       "Logistic Regression    0.636986  0.052743  0.019505\n",
       "SVM                    0.582541  0.094152  0.033499\n",
       "Gaussian Naive Bayes   0.581306  0.045122  0.017064\n",
       "AdaBoost               0.541544  0.119302  0.041342\n",
       "Bernoulli Naive Bayes  0.461730  0.098944  0.040726\n",
       "Random Forest          0.286704  0.184333  0.063496"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing = Pipeline([('imp', imp),('scale', Normalizer()),('pca', PCA(n_components = 200))])\n",
    "scores = cv_test(X_train_orig, y_train, preprocessing, classifiers, clf_names)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "It appears that logistic regression and SVM's are unaffected by PCA, while Bernoulli NB and random forest are hurt the most. Interestingly, the performance of Gaussian NB is substantially improved and more stable in a lower dimensional projection.\n",
    "\n",
    "We set a baseline F1-score at 0.629 based on AdaBoost's performance without PCA, but given the stable performance of logistic regression both with and without PCA, it appears more promising, so we focus our efforts on developing features best suited for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Adding new features\n",
    "### Sentiment intensity from thermometer and index features\n",
    "In our exploratory data analysis, we noticed that a number of \"thermometer\" features showed promise for a linear model if they were transformed to measure intensity of sentiment (the distance from a neutral sentiment score of 50) rather than merely positive or negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def add_thermometer_intensity(df):\n",
    "    \n",
    "    def thermometer_to_intensity(x):\n",
    "        if x == np.nan:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return abs(x - 5) ** 2\n",
    "    \n",
    "    columns_to_convert = (df.max() >= 10) & (df.min() == 0)\n",
    "    columns_to_convert[[df.columns.get_loc('VCF0114_r1'),df.columns.get_loc('VCF1015')]] = False\n",
    "    thermometer_df = df.loc[:,columns_to_convert]\n",
    "    thermometer_df = thermometer_df.applymap(thermometer_to_intensity)\n",
    "    thermometer_df.columns = thermometer_df.columns + '_int'\n",
    "    thermometer_df['int_sum_therm'] = thermometer_df.sum(axis = 1) ** 1.2\n",
    "    return pd.concat([df, thermometer_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>range</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.628258</td>\n",
       "      <td>0.051682</td>\n",
       "      <td>0.018574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernoulli Naive Bayes</th>\n",
       "      <td>0.616842</td>\n",
       "      <td>0.076792</td>\n",
       "      <td>0.028482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.613256</td>\n",
       "      <td>0.023380</td>\n",
       "      <td>0.008680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.567461</td>\n",
       "      <td>0.084024</td>\n",
       "      <td>0.029659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes</th>\n",
       "      <td>0.551443</td>\n",
       "      <td>0.189017</td>\n",
       "      <td>0.068835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.508697</td>\n",
       "      <td>0.126133</td>\n",
       "      <td>0.043670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           mean     range       std\n",
       "AdaBoost               0.628258  0.051682  0.018574\n",
       "Bernoulli Naive Bayes  0.616842  0.076792  0.028482\n",
       "Logistic Regression    0.613256  0.023380  0.008680\n",
       "SVM                    0.567461  0.084024  0.029659\n",
       "Gaussian Naive Bayes   0.551443  0.189017  0.068835\n",
       "Random Forest          0.508697  0.126133  0.043670"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = add_thermometer_intensity(X_train_orig)\n",
    "preprocessing = Pipeline([('imp', Imputer(missing_values = 'NaN', strategy = 'median')),('scale', Normalizer())])\n",
    "scores = cv_test(X_train, y_train, preprocessing = preprocessing, classifiers = classifiers, clf_names = clf_names)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Unfortunately, it appears that, by itself, the inclusion of these features is largely counterproductive. A potential explanation may be that the trends, while present, are not strong enough to contribute an improvement to the model and may have in fact added even more noise. It may remain possible, however, that a small subset of these newly generated features are deemed significant by RFE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Intensity from ordinal features\n",
    "Below, we reset the data and try for a select group of ordinal features a similar idea as that for the \"thermometer\" features. Again, we observe a similar result akin to adding more noise to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def add_ordinal_intensity(df):\n",
    "    columns_to_convert = ['VCF0803','VCF0806','VCF0830','VCF0851','VCF9014','VCF9015','VCF9039','VCF9042','VCF0301',\n",
    "                     'VCF0303','VCF0502','VCF0604','VCF0605','VCF0880a','VCF9009','VCF9045']\n",
    "    intensity_df = df.loc[:,columns_to_convert]\n",
    "    intensity_df = abs(intensity_df - (intensity_df.max() + intensity_df.min()) / 2)\n",
    "    intensity_df.columns = intensity_df.columns + '_int'\n",
    "    intensity_df['int_sum_ord'] = intensity_df.sum(axis = 1) ** 2\n",
    "    return pd.concat([df, intensity_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>range</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.633885</td>\n",
       "      <td>0.050391</td>\n",
       "      <td>0.019051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.617621</td>\n",
       "      <td>0.072399</td>\n",
       "      <td>0.030440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernoulli Naive Bayes</th>\n",
       "      <td>0.617616</td>\n",
       "      <td>0.070770</td>\n",
       "      <td>0.023512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.575348</td>\n",
       "      <td>0.069758</td>\n",
       "      <td>0.029827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes</th>\n",
       "      <td>0.551102</td>\n",
       "      <td>0.190731</td>\n",
       "      <td>0.071850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.490096</td>\n",
       "      <td>0.125736</td>\n",
       "      <td>0.044157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           mean     range       std\n",
       "AdaBoost               0.633885  0.050391  0.019051\n",
       "Logistic Regression    0.617621  0.072399  0.030440\n",
       "Bernoulli Naive Bayes  0.617616  0.070770  0.023512\n",
       "SVM                    0.575348  0.069758  0.029827\n",
       "Gaussian Naive Bayes   0.551102  0.190731  0.071850\n",
       "Random Forest          0.490096  0.125736  0.044157"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = add_ordinal_intensity(X_train_orig)\n",
    "scores = cv_test(X_train, y_train, preprocessing, classifiers, clf_names)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Summing \"don't know\" responses\n",
    "We also prevoiusly observed that the more a respondent answered \"don't know\" to a variety of questions, the more likely he/she is to be a non-voter. However, a simple sum of all \"don't know\" responses contributes no new information to a linear model, so we instead square it to prevent a potential collinearity issue and see if it makes any meaningful contributions to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def add_dk_sum(df):\n",
    "    df = df[:]\n",
    "    dk_column_index = df.columns.str.contains('dk')\n",
    "    dk_df = df.loc[:,dk_column_index]\n",
    "    df['dk_sum'] = dk_df.sum(axis = 1) ** 2\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>range</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.625848</td>\n",
       "      <td>0.077885</td>\n",
       "      <td>0.030249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.619343</td>\n",
       "      <td>0.057697</td>\n",
       "      <td>0.023684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernoulli Naive Bayes</th>\n",
       "      <td>0.613230</td>\n",
       "      <td>0.075992</td>\n",
       "      <td>0.026451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.573132</td>\n",
       "      <td>0.074040</td>\n",
       "      <td>0.031025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes</th>\n",
       "      <td>0.551333</td>\n",
       "      <td>0.195351</td>\n",
       "      <td>0.072402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.520858</td>\n",
       "      <td>0.128579</td>\n",
       "      <td>0.043587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           mean     range       std\n",
       "Logistic Regression    0.625848  0.077885  0.030249\n",
       "AdaBoost               0.619343  0.057697  0.023684\n",
       "Bernoulli Naive Bayes  0.613230  0.075992  0.026451\n",
       "SVM                    0.573132  0.074040  0.031025\n",
       "Gaussian Naive Bayes   0.551333  0.195351  0.072402\n",
       "Random Forest          0.520858  0.128579  0.043587"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = add_dk_sum(X_train_orig)\n",
    "scores = cv_test(X_train, y_train, preprocessing, classifiers, clf_names)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Again, by itself, the inclusion of this new feature appears not to improve the performance of these classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Removing features\n",
    "### First one-hot features\n",
    "In one-hot encoding of categorical variables, it is often advisable to drop one column of the one-hot set to avoid collinearity due to redundant data. It is much the same principle as a binary feature being represented with one column as opposed to two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def drop_first_onehot(df):\n",
    "    return df.loc[:, ~df.columns.str.contains('oh0')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Correlated features\n",
    "Because a linear model appears to work best for this data, it is important to address correlated features to prevent collinearity issues. Correlated features can potentially render the coefficients of linear models uninterpretable as well as reduce the efficacy of feature selection algorithms.\n",
    "\n",
    "Below is a function that identifies families of correlated features and one-by-one removes the least predictive feature of each family until the correlation chain is broken. We set the threshhold so that all features with correlations above 0.85 are flagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def break_correlation(in_X, y, threshhold = 0.85, scoring = mutual_info_score):\n",
    "       \n",
    "    X = pd.DataFrame(Imputer(missing_values='NaN', strategy = 'median').fit_transform(in_X), columns = in_X.columns)\n",
    "    \n",
    "    corr_mask = abs(X.corr()) > threshhold\n",
    "    corr_list = corr_mask.sum()\n",
    "    corr_list = corr_list[corr_list > 1]\n",
    "    corr_dict = dict(corr_list)\n",
    "    \n",
    "    print('Correlated features:', len(corr_dict))\n",
    "    \n",
    "    cluster_list = []\n",
    "    while len(corr_list) > 0:\n",
    "        cluster = dict()\n",
    "        key = corr_list.index[0]\n",
    "        correlations = list(corr_mask[key][corr_mask[key]].index)\n",
    "\n",
    "        i = 1\n",
    "        while i < len(correlations):\n",
    "            key = correlations[i]\n",
    "            temp_correlations = list(corr_mask[key][corr_mask[key]].index)\n",
    "            difference = list(set(temp_correlations) - set(correlations))\n",
    "            if len(difference) != 0:\n",
    "                correlations = correlations + difference\n",
    "            i = i + 1\n",
    "            \n",
    "        for i in range(0, len(correlations)):\n",
    "            cluster[correlations[i]] = corr_dict[correlations[i]]\n",
    "            corr_dict.pop(correlations[i])\n",
    "            corr_list.pop(correlations[i])\n",
    "            \n",
    "        cluster_list.append(cluster)\n",
    "        \n",
    "    removed = []\n",
    "        \n",
    "    for cluster in cluster_list:\n",
    "        scores = dict()\n",
    "        for feature in cluster:\n",
    "            scores[feature] = scoring(X[feature], y)\n",
    "\n",
    "        while(len(cluster) > 0):\n",
    "            min_key = min(scores, key = scores.get)\n",
    "            removed.append(min_key)\n",
    "            temp_correlations = list(corr_mask[min_key][corr_mask[min_key]].index)\n",
    "            temp_correlations = list(set(temp_correlations).intersection(cluster))\n",
    "            temp_correlations.remove(min_key)\n",
    "            for each in temp_correlations:\n",
    "                if cluster[each] > 2:\n",
    "                    cluster[each] = cluster[each] - 1\n",
    "                else:\n",
    "                    cluster.pop(each)\n",
    "                    scores.pop(each)\n",
    "                                \n",
    "            cluster.pop(min_key)\n",
    "            scores.pop(min_key)\n",
    "        \n",
    "    print('Removed {} features:\\n'.format(len(removed)),removed)        \n",
    "    return in_X.drop(removed, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlated features: 51\n",
      "Removed 27 features:\n",
      " ['VCF0107_oh5', 'VCF0108', 'VCF0112_oh2', 'VCF0127', 'VCF0450', 'VCF9009', 'VCF0742', 'VCF0905', 'VCF9030_oh5', 'VCF0224', 'VCF0425_dk', 'VCF0624', 'VCF0114_r2', 'VCF0504_dk', 'VCF0513_dk', 'VCF0541_dk', 'VCF0549_dk', 'VCF0804_dk', 'VCF0803_dk', 'VCF0804', 'VCF0110', 'VCF0303', 'VCF0870', 'VCF0714_oh1', 'VCF9131_oh2', 'VCF9132_oh1', 'VCF9133_oh2']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>range</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.628454</td>\n",
       "      <td>0.080205</td>\n",
       "      <td>0.032918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.627084</td>\n",
       "      <td>0.064311</td>\n",
       "      <td>0.021225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernoulli Naive Bayes</th>\n",
       "      <td>0.623354</td>\n",
       "      <td>0.089485</td>\n",
       "      <td>0.030822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes</th>\n",
       "      <td>0.587230</td>\n",
       "      <td>0.080056</td>\n",
       "      <td>0.026131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.571254</td>\n",
       "      <td>0.093152</td>\n",
       "      <td>0.032328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.495484</td>\n",
       "      <td>0.060956</td>\n",
       "      <td>0.019989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           mean     range       std\n",
       "Logistic Regression    0.628454  0.080205  0.032918\n",
       "AdaBoost               0.627084  0.064311  0.021225\n",
       "Bernoulli Naive Bayes  0.623354  0.089485  0.030822\n",
       "Gaussian Naive Bayes   0.587230  0.080056  0.026131\n",
       "SVM                    0.571254  0.093152  0.032328\n",
       "Random Forest          0.495484  0.060956  0.019989"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = drop_first_onehot(X_train_orig)\n",
    "X_train = break_correlation(X_train, y_train)\n",
    "scores = cv_test(X_train, y_train, preprocessing, classifiers, clf_names)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "From running another set of cross validation tests, we see that the performance of various classifiers was not affected significantly by removing collinear features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Recursive feature elimination\n",
    "Because the number of training examples is limited relative to the size of the feature set, classification models may be prone to overfit the data, reducing their accuracy on test examples. Furthermore, a number of these features may be noisy relative to the target feature, so we require a systematic means of determining which subset of features will yield the best performance.\n",
    "\n",
    "Recursive feature elimination (RFE) utilizes a user-specified classification algorithm and scoring metric to find this subset. By specifying a linear model like logistic regression that learns coefficients for each feature, RFE will first use cross validation to assess the performance of the model on the whole feature set, remove those features with the smallest coefficients, and then asses the performance of the model on the new subset. This process is performed recursively until the optimal number of features to keep is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def feature_elimination(X, y, preprocessing, estimator = LogisticRegression()):\n",
    "    X = preprocessing.fit_transform(X)\n",
    "    rfecv = RFECV(estimator = estimator, step = 5, cv = StratifiedKFold(3), scoring = 'f1')\n",
    "    return rfecv.fit_transform(X, y), rfecv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first perform RFE with logistic regression on the original feature set to establish a baseline for RFE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features selected: 115\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>range</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.670029</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.023812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.639338</td>\n",
       "      <td>0.075126</td>\n",
       "      <td>0.024933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernoulli Naive Bayes</th>\n",
       "      <td>0.636934</td>\n",
       "      <td>0.092236</td>\n",
       "      <td>0.029511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.603206</td>\n",
       "      <td>0.069022</td>\n",
       "      <td>0.025486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.594677</td>\n",
       "      <td>0.121343</td>\n",
       "      <td>0.039415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes</th>\n",
       "      <td>0.594633</td>\n",
       "      <td>0.031373</td>\n",
       "      <td>0.013054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           mean     range       std\n",
       "Logistic Regression    0.670029  0.060000  0.023812\n",
       "AdaBoost               0.639338  0.075126  0.024933\n",
       "Bernoulli Naive Bayes  0.636934  0.092236  0.029511\n",
       "Random Forest          0.603206  0.069022  0.025486\n",
       "SVM                    0.594677  0.121343  0.039415\n",
       "Gaussian Naive Bayes   0.594633  0.031373  0.013054"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train_orig\n",
    "columns = X_train.columns\n",
    "X_train, rfecv = feature_elimination(X_train, y_train, preprocessing, LogisticRegression())\n",
    "print('Number of features selected:', rfecv.n_features_)\n",
    "scores = cv_test(X_train, y_train, None, classifiers, clf_names)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 366 features, RFE selected 115 features, resulting in improved F1 scores across the board, especially for logistic regression and random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Most important features\n",
    "With this optimized feature set, we can train a logistic regression model and examine the resulting coefficients to determine the importance of each feature. Before training the model however, we remove redundantly correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlated features: 11\n",
      "Removed 6 features:\n",
      " ['VCF0107_oh5', 'VCF0108', 'VCF0127', 'VCF0224', 'VCF0425_dk', 'VCF0504_dk']\n"
     ]
    }
   ],
   "source": [
    "selected_features = columns[rfecv.ranking_ == 1]\n",
    "X_rfe = break_correlation(pd.DataFrame(X_train, columns = selected_features), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>feature</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2.305570</td>\n",
       "      <td>VCF0713_oh4</td>\n",
       "      <td>455.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.236885</td>\n",
       "      <td>VCF0223_dk</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1.095684</td>\n",
       "      <td>VCF9017_dk</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.069919</td>\n",
       "      <td>VCF0233_dk</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1.043036</td>\n",
       "      <td>VCF0870_dk</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1.002865</td>\n",
       "      <td>VCF0713_oh1</td>\n",
       "      <td>1814.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.995050</td>\n",
       "      <td>VCF9133_oh0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.955841</td>\n",
       "      <td>VCF0871_dk</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.890447</td>\n",
       "      <td>VCF0147_oh0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.845162</td>\n",
       "      <td>VCF0107_oh4</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.814055</td>\n",
       "      <td>VCF0713_oh3</td>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.797949</td>\n",
       "      <td>VCF0224_dk</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.789993</td>\n",
       "      <td>VCF9132_oh3</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.789361</td>\n",
       "      <td>VCF0204_dk</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.788437</td>\n",
       "      <td>VCF0127</td>\n",
       "      <td>523.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.785969</td>\n",
       "      <td>VCF9015_dk</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.783534</td>\n",
       "      <td>VCF0852_dk</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.723894</td>\n",
       "      <td>VCF0105a_oh2</td>\n",
       "      <td>732.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.717967</td>\n",
       "      <td>VCF0904_oh0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.717468</td>\n",
       "      <td>VCF9131_oh0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.713295</td>\n",
       "      <td>VCF0713_oh2</td>\n",
       "      <td>1222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.674755</td>\n",
       "      <td>VCF0413_dk</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.646479</td>\n",
       "      <td>VCF0146</td>\n",
       "      <td>2538.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.637622</td>\n",
       "      <td>VCF9030a</td>\n",
       "      <td>1573.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.628754</td>\n",
       "      <td>VCF9048_dk</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.622900</td>\n",
       "      <td>VCF9056_dk</td>\n",
       "      <td>337.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.604457</td>\n",
       "      <td>VCF0906_dk</td>\n",
       "      <td>770.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.601906</td>\n",
       "      <td>VCF9132_oh0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.600101</td>\n",
       "      <td>VCF0105a_oh3</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.588957</td>\n",
       "      <td>VCF0147_oh4</td>\n",
       "      <td>139.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.588189</td>\n",
       "      <td>VCF0426_dk</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.583354</td>\n",
       "      <td>VCF0227_dk</td>\n",
       "      <td>162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.565707</td>\n",
       "      <td>VCF0838_dk</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.561896</td>\n",
       "      <td>VCF0740</td>\n",
       "      <td>305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.559650</td>\n",
       "      <td>VCF9030_oh4</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.542388</td>\n",
       "      <td>VCF0801_dk</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.533953</td>\n",
       "      <td>VCF0224</td>\n",
       "      <td>183850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.525183</td>\n",
       "      <td>VCF0220_dk</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.522418</td>\n",
       "      <td>VCF0219_dk</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.514070</td>\n",
       "      <td>VCF9049_dk</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.508728</td>\n",
       "      <td>VCF0549_dk</td>\n",
       "      <td>138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.507948</td>\n",
       "      <td>VCF0504_dk</td>\n",
       "      <td>367.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.507821</td>\n",
       "      <td>VCF0107_oh6</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.485026</td>\n",
       "      <td>VCF0513_dk</td>\n",
       "      <td>164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.484042</td>\n",
       "      <td>VCF0713_oh5</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.463668</td>\n",
       "      <td>VCF0894_dk</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.462599</td>\n",
       "      <td>VCF0412_dk</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.459890</td>\n",
       "      <td>VCF9041_dk</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.457117</td>\n",
       "      <td>VCF0907_dk</td>\n",
       "      <td>965.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.455010</td>\n",
       "      <td>VCF0624_dk</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     coefficient       feature       sum\n",
       "104     2.305570   VCF0713_oh4     455.0\n",
       "24      1.236885    VCF0223_dk      89.0\n",
       "68      1.095684    VCF9017_dk      13.0\n",
       "30      1.069919    VCF0233_dk      68.0\n",
       "80      1.043036    VCF0870_dk      12.0\n",
       "101     1.002865   VCF0713_oh1    1814.0\n",
       "114     0.995050   VCF9133_oh0      12.0\n",
       "81      0.955841    VCF0871_dk      22.0\n",
       "96      0.890447   VCF0147_oh0      15.0\n",
       "89      0.845162   VCF0107_oh4       9.0\n",
       "103     0.814055   VCF0713_oh3     168.0\n",
       "25      0.797949    VCF0224_dk      96.0\n",
       "113     0.789993   VCF9132_oh3      89.0\n",
       "17      0.789361    VCF0204_dk     101.0\n",
       "2       0.788437       VCF0127     523.0\n",
       "66      0.785969    VCF9015_dk      15.0\n",
       "63      0.783534    VCF0852_dk       9.0\n",
       "85      0.723894  VCF0105a_oh2     732.0\n",
       "107     0.717967   VCF0904_oh0      40.0\n",
       "109     0.717468   VCF9131_oh0      14.0\n",
       "102     0.713295   VCF0713_oh2    1222.0\n",
       "35      0.674755    VCF0413_dk      43.0\n",
       "3       0.646479       VCF0146    2538.0\n",
       "15      0.637622      VCF9030a    1573.0\n",
       "60      0.628754    VCF9048_dk      61.0\n",
       "46      0.622900    VCF9056_dk     337.0\n",
       "43      0.604457    VCF0906_dk     770.0\n",
       "112     0.601906   VCF9132_oh0      11.0\n",
       "86      0.600101  VCF0105a_oh3      65.0\n",
       "98      0.588957   VCF0147_oh4     139.0\n",
       "40      0.588189    VCF0426_dk      40.0\n",
       "27      0.583354    VCF0227_dk     162.0\n",
       "78      0.565707    VCF0838_dk      29.0\n",
       "12      0.561896       VCF0740     305.0\n",
       "108     0.559650   VCF9030_oh4      42.0\n",
       "47      0.542388    VCF0801_dk     132.0\n",
       "26      0.533953       VCF0224  183850.0\n",
       "23      0.525183    VCF0220_dk     101.0\n",
       "22      0.522418    VCF0219_dk      40.0\n",
       "61      0.514070    VCF9049_dk      47.0\n",
       "53      0.508728    VCF0549_dk     138.0\n",
       "50      0.507948    VCF0504_dk     367.0\n",
       "91      0.507821   VCF0107_oh6       7.0\n",
       "51      0.485026    VCF0513_dk     164.0\n",
       "105     0.484042   VCF0713_oh5     102.0\n",
       "58      0.463668    VCF0894_dk      47.0\n",
       "34      0.462599    VCF0412_dk      73.0\n",
       "70      0.459890    VCF9041_dk      24.0\n",
       "44      0.457117    VCF0907_dk     965.0\n",
       "77      0.455010    VCF0624_dk      16.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_rankings = pd.concat([pd.DataFrame(lr.coef_.T), pd.DataFrame(selected_features)], axis = 1)\n",
    "feature_rankings.columns = ['coefficient','feature']\n",
    "feature_rankings.coefficient = feature_rankings.coefficient.apply(lambda x: abs(x))\n",
    "feature_rankings['sum'] = feature_rankings.feature.apply(lambda x: sum(df_train.loc[:,x].dropna()))\n",
    "feature_rankings.sort_values(by = 'coefficient', ascending = False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table displays the features ranked by the absolute values of their associated logistic regression coefficients in descending order. Because the vast majority of the listed features are binary, the sum column provides a quick means of seeing how many respondents belong to that feature category. We immediately notice some important points:\n",
    "1. The model places higher importance on binary features and relies very little on continuous or ordinal features\n",
    "2. A surprisingly high number of \"don't know\" features were selected by RFE, many of which are quite sparse. This means that many features might be quite helpful for classification, but only for a very small subset of respondents.\n",
    "3. The most important feature is also the least interesting; VCF0713_oh4 consists of respondents who, in their pre-election interviews, indicated their intent not to vote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Classifying with VCF0713_oh4 as only feature\n",
    "Given the importance the model places on VCF0713_oh4 (respondent answered \"Do not intend to vote\" to \"Who do you plan to vote for?\"), we check to see how the logistic regression performs with that as its only feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score: 0.591724784171\n"
     ]
    }
   ],
   "source": [
    "df = df_orig[:]\n",
    "X_train = df[df.year < 2012]\n",
    "score_cv = cross_val_score(lr, X = X_train.VCF0713_oh4.reshape(-1,1), y = y_train, scoring = 'f1', cv = 10)\n",
    "print('f1-score:', score_cv.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite a high f1-score for only one feature, and it begs the question about whether the remaining features provide any useful information for classification purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Classifying with everything else\n",
    "To check if the rest of the features remain meaningful without the inclusion of VCF0713_oh4, we perform logistic regression with it excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54494865938123405"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_orig[:]\n",
    "X_train = df[df.year < 2012]\n",
    "X_train = X_train.drop(['VCF0713_oh4','VCF0713_oh3','VCF0713_oh2','VCF0713_oh1','VCF0702'], axis = 1)\n",
    "X_exp = preprocessing.fit_transform(X_train)\n",
    "score_cv = cross_val_score(lr, X = X_exp, y = y_train, scoring = 'f1', cv = 10)\n",
    "score_cv.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the performance of logistic regression on the remaining features is not as good as that on just VCF0713_oh4 alone, it is still an encouraging sign that the performance is not significantly lower. That the inclusion of the rest of the feature set results in an approximately .08 improvement in the F1 score suggests that there exists some useful diversity in the information contained within the remaining features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Testing Combinations\n",
    "So far we have seen that performing RFE on the dataset leads to the greatest improvement in the F1 score while individual feature set modifications tend to hurt performance or leave it unaffected. However, it could be the case that some of the modifications do contribute useful information, but the gains are outweighed by an expanding feature space and the addition of many more unhelpful features. In this situation, the benefit of a modification to the feature set may not be apparent until after RFE has been applied on the modified data. Furthermore, with certain types of data, RFE can be unstable and sensitive to tiny changes in the dataset. Thus, we test the effect of RFE on all combinations of feature modifications for thoroughness.\n",
    "### Optimizing feature sets for voting classifiers\n",
    "Based on previous tests, it appears that the various algorithms have fairly similar predictive power even though some algorithms are fundamentally quite different from others. This suggests that a voting classifier may be worth trying later on, so during the tests of feature modification combinations, we also keep track of which combinations produce the best performance for each algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operations: []\n",
      "Number of features selected: 115\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.670029  0.060000  0.023812\n",
      "AdaBoost               0.639338  0.075126  0.024933\n",
      "Bernoulli Naive Bayes  0.636934  0.092236  0.029511\n",
      "SVM                    0.594677  0.121343  0.039415\n",
      "\n",
      "Correlated features: 51\n",
      "Removed 27 features:\n",
      " ['VCF0107_oh5', 'VCF0108', 'VCF0112_oh2', 'VCF0127', 'VCF0450', 'VCF9009', 'VCF0742', 'VCF0905', 'VCF9030_oh5', 'VCF0224', 'VCF0425_dk', 'VCF0624', 'VCF0114_r2', 'VCF0504_dk', 'VCF0513_dk', 'VCF0541_dk', 'VCF0549_dk', 'VCF0804_dk', 'VCF0803_dk', 'VCF0804', 'VCF0110', 'VCF0303', 'VCF0870', 'VCF0714_oh1', 'VCF9131_oh2', 'VCF9132_oh1', 'VCF9133_oh2']\n",
      "Operations: ['break correlation']\n",
      "Number of features selected: 113\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.667294  0.076280  0.032191\n",
      "Bernoulli Naive Bayes  0.645852  0.074378  0.025399\n",
      "AdaBoost               0.644696  0.035340  0.012411\n",
      "SVM                    0.594256  0.121471  0.039542\n",
      "\n",
      "Operations: ['drop first one-hot']\n",
      "Number of features selected: 122\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.665271  0.060712  0.027846\n",
      "Bernoulli Naive Bayes  0.640967  0.085796  0.027344\n",
      "AdaBoost               0.640054  0.062771  0.021677\n",
      "SVM                    0.594208  0.118996  0.038705\n",
      "\n",
      "Correlated features: 51\n",
      "Removed 27 features:\n",
      " ['VCF0107_oh5', 'VCF0108', 'VCF0112_oh2', 'VCF0127', 'VCF0450', 'VCF9009', 'VCF0742', 'VCF0905', 'VCF9030_oh5', 'VCF0224', 'VCF0425_dk', 'VCF0624', 'VCF0114_r2', 'VCF0504_dk', 'VCF0513_dk', 'VCF0541_dk', 'VCF0549_dk', 'VCF0804_dk', 'VCF0803_dk', 'VCF0804', 'VCF0110', 'VCF0303', 'VCF0870', 'VCF0714_oh1', 'VCF9131_oh2', 'VCF9132_oh1', 'VCF9133_oh2']\n",
      "Operations: ['drop first one-hot', 'break correlation']\n",
      "Number of features selected: 120\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.669303  0.065476  0.027760\n",
      "AdaBoost               0.648410  0.035918  0.014084\n",
      "Bernoulli Naive Bayes  0.644047  0.069844  0.023174\n",
      "SVM                    0.593295  0.116667  0.038091\n",
      "\n",
      "Operations: ['add dk sum']\n",
      "Number of features selected: 111\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.665931  0.067552  0.026095\n",
      "Bernoulli Naive Bayes  0.640184  0.071689  0.025889\n",
      "AdaBoost               0.639395  0.080359  0.027868\n",
      "SVM                    0.593706  0.116486  0.037957\n",
      "\n",
      "Correlated features: 51\n",
      "Removed 27 features:\n",
      " ['VCF0107_oh5', 'VCF0108', 'VCF0112_oh2', 'VCF0127', 'VCF0450', 'VCF9009', 'VCF0742', 'VCF0905', 'VCF9030_oh5', 'VCF0224', 'VCF0425_dk', 'VCF0624', 'VCF0114_r2', 'VCF0504_dk', 'VCF0513_dk', 'VCF0541_dk', 'VCF0549_dk', 'VCF0804_dk', 'VCF0803_dk', 'VCF0804', 'VCF0110', 'VCF0303', 'VCF0870', 'VCF0714_oh1', 'VCF9131_oh2', 'VCF9132_oh1', 'VCF9133_oh2']\n",
      "Operations: ['add dk sum', 'break correlation']\n",
      "Number of features selected: 114\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.668977  0.079777  0.029555\n",
      "AdaBoost               0.647818  0.051780  0.019380\n",
      "Bernoulli Naive Bayes  0.645473  0.086070  0.028885\n",
      "SVM                    0.594677  0.121343  0.039415\n",
      "\n",
      "Operations: ['add dk sum', 'drop first one-hot']\n",
      "Number of features selected: 83\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.667233  0.036248  0.014770\n",
      "Bernoulli Naive Bayes  0.656224  0.056554  0.019810\n",
      "AdaBoost               0.651466  0.057835  0.019714\n",
      "SVM                    0.591725  0.111594  0.036071\n",
      "\n",
      "Correlated features: 51\n",
      "Removed 27 features:\n",
      " ['VCF0107_oh5', 'VCF0108', 'VCF0112_oh2', 'VCF0127', 'VCF0450', 'VCF9009', 'VCF0742', 'VCF0905', 'VCF9030_oh5', 'VCF0224', 'VCF0425_dk', 'VCF0624', 'VCF0114_r2', 'VCF0504_dk', 'VCF0513_dk', 'VCF0541_dk', 'VCF0549_dk', 'VCF0804_dk', 'VCF0803_dk', 'VCF0804', 'VCF0110', 'VCF0303', 'VCF0870', 'VCF0714_oh1', 'VCF9131_oh2', 'VCF9132_oh1', 'VCF9133_oh2']\n",
      "Operations: ['add dk sum', 'drop first one-hot', 'break correlation']\n",
      "Number of features selected: 131\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.660827  0.070492  0.027631\n",
      "Bernoulli Naive Bayes  0.647364  0.087645  0.031908\n",
      "AdaBoost               0.637173  0.055199  0.019551\n",
      "SVM                    0.597687  0.109142  0.035843\n",
      "\n",
      "Operations: ['add ordinal intensity']\n",
      "Number of features selected: 127\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.662359  0.050680  0.017949\n",
      "Bernoulli Naive Bayes  0.633397  0.080119  0.028707\n",
      "AdaBoost               0.632504  0.062692  0.020324\n",
      "SVM                    0.595751  0.121343  0.039286\n",
      "\n",
      "Correlated features: 53\n",
      "Removed 28 features:\n",
      " ['VCF0107_oh5', 'VCF0108', 'VCF0112_oh2', 'VCF0127', 'VCF0450', 'VCF9009', 'VCF0742', 'VCF0905', 'VCF9030_oh5', 'VCF0224', 'VCF0425_dk', 'VCF0624', 'VCF0114_r2', 'VCF0504_dk', 'VCF0513_dk', 'VCF0541_dk', 'VCF0549_dk', 'VCF0804_dk', 'VCF0803_dk', 'VCF0804', 'VCF0110', 'VCF0303', 'VCF0301_int', 'VCF0870', 'VCF0714_oh1', 'VCF9131_oh2', 'VCF9132_oh1', 'VCF9133_oh2']\n",
      "Operations: ['add ordinal intensity', 'break correlation']\n",
      "Number of features selected: 99\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.667945  0.041246  0.014576\n",
      "AdaBoost               0.651894  0.075114  0.026618\n",
      "Bernoulli Naive Bayes  0.648412  0.074595  0.024061\n",
      "SVM                    0.592728  0.111594  0.036535\n",
      "\n",
      "Operations: ['add ordinal intensity', 'drop first one-hot']\n",
      "Number of features selected: 99\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.658260  0.058654  0.019432\n",
      "AdaBoost               0.640760  0.054626  0.021117\n",
      "Bernoulli Naive Bayes  0.640197  0.055058  0.019638\n",
      "SVM                    0.592728  0.111594  0.036535\n",
      "\n",
      "Correlated features: 53\n",
      "Removed 28 features:\n",
      " ['VCF0107_oh5', 'VCF0108', 'VCF0112_oh2', 'VCF0127', 'VCF0450', 'VCF9009', 'VCF0742', 'VCF0905', 'VCF9030_oh5', 'VCF0224', 'VCF0425_dk', 'VCF0624', 'VCF0114_r2', 'VCF0504_dk', 'VCF0513_dk', 'VCF0541_dk', 'VCF0549_dk', 'VCF0804_dk', 'VCF0803_dk', 'VCF0804', 'VCF0110', 'VCF0303', 'VCF0301_int', 'VCF0870', 'VCF0714_oh1', 'VCF9131_oh2', 'VCF9132_oh1', 'VCF9133_oh2']\n",
      "Operations: ['add ordinal intensity', 'drop first one-hot', 'break correlation']\n",
      "Number of features selected: 121\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.664653  0.076388  0.025829\n",
      "Bernoulli Naive Bayes  0.653033  0.091159  0.032342\n",
      "AdaBoost               0.637989  0.045083  0.015858\n",
      "SVM                    0.596993  0.121471  0.039162\n",
      "\n",
      "Operations: ['add ordinal intensity', 'add dk sum']\n",
      "Number of features selected: 103\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.674716  0.060313  0.021137\n",
      "AdaBoost               0.655139  0.060966  0.021572\n",
      "Bernoulli Naive Bayes  0.640523  0.085344  0.029451\n",
      "SVM                    0.593706  0.116486  0.037957\n",
      "\n",
      "Correlated features: 53\n",
      "Removed 28 features:\n",
      " ['VCF0107_oh5', 'VCF0108', 'VCF0112_oh2', 'VCF0127', 'VCF0450', 'VCF9009', 'VCF0742', 'VCF0905', 'VCF9030_oh5', 'VCF0224', 'VCF0425_dk', 'VCF0624', 'VCF0114_r2', 'VCF0504_dk', 'VCF0513_dk', 'VCF0541_dk', 'VCF0549_dk', 'VCF0804_dk', 'VCF0803_dk', 'VCF0804', 'VCF0110', 'VCF0303', 'VCF0301_int', 'VCF0870', 'VCF0714_oh1', 'VCF9131_oh2', 'VCF9132_oh1', 'VCF9133_oh2']\n",
      "Operations: ['add ordinal intensity', 'add dk sum', 'break correlation']\n",
      "Number of features selected: 115\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.668654  0.084528  0.029203\n",
      "Bernoulli Naive Bayes  0.644985  0.059983  0.021645\n",
      "AdaBoost               0.643872  0.063243  0.021136\n",
      "SVM                    0.594230  0.121343  0.039503\n",
      "\n",
      "Operations: ['add ordinal intensity', 'add dk sum', 'drop first one-hot']\n",
      "Number of features selected: 130\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.661407  0.051391  0.016366\n",
      "AdaBoost               0.644811  0.039707  0.013162\n",
      "Bernoulli Naive Bayes  0.636264  0.070599  0.024442\n",
      "SVM                    0.594540  0.114353  0.037347\n",
      "\n",
      "Correlated features: 53\n",
      "Removed 28 features:\n",
      " ['VCF0107_oh5', 'VCF0108', 'VCF0112_oh2', 'VCF0127', 'VCF0450', 'VCF9009', 'VCF0742', 'VCF0905', 'VCF9030_oh5', 'VCF0224', 'VCF0425_dk', 'VCF0624', 'VCF0114_r2', 'VCF0504_dk', 'VCF0513_dk', 'VCF0541_dk', 'VCF0549_dk', 'VCF0804_dk', 'VCF0803_dk', 'VCF0804', 'VCF0110', 'VCF0303', 'VCF0301_int', 'VCF0870', 'VCF0714_oh1', 'VCF9131_oh2', 'VCF9132_oh1', 'VCF9133_oh2']\n",
      "Operations: ['add ordinal intensity', 'add dk sum', 'drop first one-hot', 'break correlation']\n",
      "Number of features selected: 107\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.671919  0.084758  0.029909\n",
      "Bernoulli Naive Bayes  0.651863  0.055476  0.018332\n",
      "AdaBoost               0.649052  0.056538  0.021349\n",
      "SVM                    0.595642  0.126165  0.040902\n",
      "\n",
      "Operations: ['add thermometer intensity']\n",
      "Number of features selected: 96\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.664979  0.060282  0.021430\n",
      "AdaBoost               0.644131  0.038353  0.014994\n",
      "Bernoulli Naive Bayes  0.640697  0.066158  0.026334\n",
      "SVM                    0.592704  0.116486  0.037538\n",
      "\n",
      "Correlated features: 55\n",
      "Removed 29 features:\n",
      " ['VCF0107_oh5', 'VCF0108', 'VCF0112_oh2', 'VCF0127', 'VCF0450', 'VCF9009', 'VCF0742', 'VCF0905', 'VCF9030_oh5', 'VCF0207_int', 'VCF0219_int', 'VCF0224', 'VCF0425_dk', 'VCF0624', 'VCF0114_r2', 'VCF0504_dk', 'VCF0513_dk', 'VCF0541_dk', 'VCF0549_dk', 'VCF0804_dk', 'VCF0803_dk', 'VCF0804', 'VCF0110', 'VCF0303', 'VCF0870', 'VCF0714_oh1', 'VCF9131_oh2', 'VCF9132_oh1', 'VCF9133_oh2']\n",
      "Operations: ['add thermometer intensity', 'break correlation']\n",
      "Number of features selected: 122\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.675756  0.059997  0.026194\n",
      "Bernoulli Naive Bayes  0.653181  0.078932  0.027553\n",
      "AdaBoost               0.632526  0.059654  0.019890\n",
      "SVM                    0.595535  0.104112  0.035429\n",
      "\n",
      "Operations: ['add thermometer intensity', 'drop first one-hot']\n",
      "Number of features selected: 88\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.662135  0.046737  0.016045\n",
      "AdaBoost               0.652017  0.039554  0.015542\n",
      "Bernoulli Naive Bayes  0.646376  0.070115  0.022348\n",
      "SVM                    0.592728  0.111594  0.036535\n",
      "\n",
      "Correlated features: 55\n",
      "Removed 29 features:\n",
      " ['VCF0107_oh5', 'VCF0108', 'VCF0112_oh2', 'VCF0127', 'VCF0450', 'VCF9009', 'VCF0742', 'VCF0905', 'VCF9030_oh5', 'VCF0207_int', 'VCF0219_int', 'VCF0224', 'VCF0425_dk', 'VCF0624', 'VCF0114_r2', 'VCF0504_dk', 'VCF0513_dk', 'VCF0541_dk', 'VCF0549_dk', 'VCF0804_dk', 'VCF0803_dk', 'VCF0804', 'VCF0110', 'VCF0303', 'VCF0870', 'VCF0714_oh1', 'VCF9131_oh2', 'VCF9132_oh1', 'VCF9133_oh2']\n",
      "Operations: ['add thermometer intensity', 'drop first one-hot', 'break correlation']\n",
      "Number of features selected: 119\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.667217  0.071022  0.026499\n",
      "Bernoulli Naive Bayes  0.655139  0.077735  0.025733\n",
      "AdaBoost               0.649009  0.047917  0.018029\n",
      "SVM                    0.596106  0.112057  0.036970\n",
      "\n",
      "Operations: ['add thermometer intensity', 'add dk sum']\n",
      "Number of features selected: 77\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.651511  0.056738  0.021978\n",
      "Bernoulli Naive Bayes  0.645611  0.070321  0.027318\n",
      "AdaBoost               0.643857  0.058233  0.018687\n",
      "SVM                    0.591725  0.111594  0.036071\n",
      "\n",
      "Correlated features: 55\n",
      "Removed 29 features:\n",
      " ['VCF0107_oh5', 'VCF0108', 'VCF0112_oh2', 'VCF0127', 'VCF0450', 'VCF9009', 'VCF0742', 'VCF0905', 'VCF9030_oh5', 'VCF0207_int', 'VCF0219_int', 'VCF0224', 'VCF0425_dk', 'VCF0624', 'VCF0114_r2', 'VCF0504_dk', 'VCF0513_dk', 'VCF0541_dk', 'VCF0549_dk', 'VCF0804_dk', 'VCF0803_dk', 'VCF0804', 'VCF0110', 'VCF0303', 'VCF0870', 'VCF0714_oh1', 'VCF9131_oh2', 'VCF9132_oh1', 'VCF9133_oh2']\n",
      "Operations: ['add thermometer intensity', 'add dk sum', 'break correlation']\n",
      "Number of features selected: 73\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.661044  0.054125  0.019068\n",
      "Bernoulli Naive Bayes  0.652258  0.080191  0.030526\n",
      "AdaBoost               0.637998  0.024409  0.009432\n",
      "SVM                    0.591725  0.111594  0.036071\n",
      "\n",
      "Operations: ['add thermometer intensity', 'add dk sum', 'drop first one-hot']\n",
      "Number of features selected: 84\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.670138  0.049658  0.019844\n",
      "AdaBoost               0.653296  0.057234  0.019798\n",
      "Bernoulli Naive Bayes  0.645783  0.076572  0.026589\n",
      "SVM                    0.592728  0.111594  0.036535\n",
      "\n",
      "Correlated features: 55\n",
      "Removed 29 features:\n",
      " ['VCF0107_oh5', 'VCF0108', 'VCF0112_oh2', 'VCF0127', 'VCF0450', 'VCF9009', 'VCF0742', 'VCF0905', 'VCF9030_oh5', 'VCF0207_int', 'VCF0219_int', 'VCF0224', 'VCF0425_dk', 'VCF0624', 'VCF0114_r2', 'VCF0504_dk', 'VCF0513_dk', 'VCF0541_dk', 'VCF0549_dk', 'VCF0804_dk', 'VCF0803_dk', 'VCF0804', 'VCF0110', 'VCF0303', 'VCF0870', 'VCF0714_oh1', 'VCF9131_oh2', 'VCF9132_oh1', 'VCF9133_oh2']\n",
      "Operations: ['add thermometer intensity', 'add dk sum', 'drop first one-hot', 'break correlation']\n",
      "Number of features selected: 140\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.665550  0.082858  0.027334\n",
      "Bernoulli Naive Bayes  0.644940  0.068880  0.028208\n",
      "AdaBoost               0.638388  0.047162  0.018558\n",
      "SVM                    0.600380  0.108514  0.035053\n",
      "\n",
      "Operations: ['add thermometer intensity', 'add ordinal intensity']\n",
      "Number of features selected: 108\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.671308  0.046091  0.019685\n",
      "Bernoulli Naive Bayes  0.652339  0.071141  0.028725\n",
      "AdaBoost               0.644782  0.061068  0.021363\n",
      "SVM                    0.593413  0.116843  0.037428\n",
      "\n",
      "Correlated features: 57\n",
      "Removed 30 features:\n",
      " ['VCF0107_oh5', 'VCF0108', 'VCF0112_oh2', 'VCF0127', 'VCF0450', 'VCF9009', 'VCF0742', 'VCF0905', 'VCF9030_oh5', 'VCF0207_int', 'VCF0219_int', 'VCF0224', 'VCF0425_dk', 'VCF0624', 'VCF0114_r2', 'VCF0504_dk', 'VCF0513_dk', 'VCF0541_dk', 'VCF0549_dk', 'VCF0804_dk', 'VCF0803_dk', 'VCF0804', 'VCF0110', 'VCF0303', 'VCF0301_int', 'VCF0870', 'VCF0714_oh1', 'VCF9131_oh2', 'VCF9132_oh1', 'VCF9133_oh2']\n",
      "Operations: ['add thermometer intensity', 'add ordinal intensity', 'break correlation']\n",
      "Number of features selected: 93\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.666572  0.074128  0.028922\n",
      "Bernoulli Naive Bayes  0.654409  0.044378  0.015720\n",
      "AdaBoost               0.648625  0.042924  0.015461\n",
      "SVM                    0.594234  0.114149  0.037809\n",
      "\n",
      "Operations: ['add thermometer intensity', 'add ordinal intensity', 'drop first one-hot']\n",
      "Number of features selected: 85\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.672624  0.043089  0.016460\n",
      "AdaBoost               0.647294  0.059117  0.019979\n",
      "Bernoulli Naive Bayes  0.643180  0.066562  0.021813\n",
      "SVM                    0.592728  0.111594  0.036535\n",
      "\n",
      "Correlated features: 57\n",
      "Removed 30 features:\n",
      " ['VCF0107_oh5', 'VCF0108', 'VCF0112_oh2', 'VCF0127', 'VCF0450', 'VCF9009', 'VCF0742', 'VCF0905', 'VCF9030_oh5', 'VCF0207_int', 'VCF0219_int', 'VCF0224', 'VCF0425_dk', 'VCF0624', 'VCF0114_r2', 'VCF0504_dk', 'VCF0513_dk', 'VCF0541_dk', 'VCF0549_dk', 'VCF0804_dk', 'VCF0803_dk', 'VCF0804', 'VCF0110', 'VCF0303', 'VCF0301_int', 'VCF0870', 'VCF0714_oh1', 'VCF9131_oh2', 'VCF9132_oh1', 'VCF9133_oh2']\n",
      "Operations: ['add thermometer intensity', 'add ordinal intensity', 'drop first one-hot', 'break correlation']\n",
      "Number of features selected: 115\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.670181  0.056550  0.022234\n",
      "Bernoulli Naive Bayes  0.655051  0.067243  0.026176\n",
      "AdaBoost               0.648688  0.054033  0.020166\n",
      "SVM                    0.592140  0.099247  0.032562\n",
      "\n",
      "Operations: ['add thermometer intensity', 'add ordinal intensity', 'add dk sum']\n",
      "Number of features selected: 94\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.661131  0.060049  0.019562\n",
      "AdaBoost               0.641836  0.055928  0.018585\n",
      "Bernoulli Naive Bayes  0.635314  0.085448  0.029787\n",
      "SVM                    0.593706  0.116486  0.037957\n",
      "\n",
      "Correlated features: 57\n",
      "Removed 30 features:\n",
      " ['VCF0107_oh5', 'VCF0108', 'VCF0112_oh2', 'VCF0127', 'VCF0450', 'VCF9009', 'VCF0742', 'VCF0905', 'VCF9030_oh5', 'VCF0207_int', 'VCF0219_int', 'VCF0224', 'VCF0425_dk', 'VCF0624', 'VCF0114_r2', 'VCF0504_dk', 'VCF0513_dk', 'VCF0541_dk', 'VCF0549_dk', 'VCF0804_dk', 'VCF0803_dk', 'VCF0804', 'VCF0110', 'VCF0303', 'VCF0301_int', 'VCF0870', 'VCF0714_oh1', 'VCF9131_oh2', 'VCF9132_oh1', 'VCF9133_oh2']\n",
      "Operations: ['add thermometer intensity', 'add ordinal intensity', 'add dk sum', 'break correlation']\n",
      "Number of features selected: 89\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.667741  0.062957  0.023684\n",
      "Bernoulli Naive Bayes  0.662728  0.044363  0.016107\n",
      "AdaBoost               0.644943  0.019156  0.007020\n",
      "SVM                    0.593706  0.116486  0.037957\n",
      "\n",
      "Operations: ['add thermometer intensity', 'add ordinal intensity', 'add dk sum', 'drop first one-hot']\n",
      "Number of features selected: 86\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.661941  0.034004  0.013036\n",
      "Bernoulli Naive Bayes  0.650860  0.060616  0.020526\n",
      "AdaBoost               0.647553  0.052799  0.019247\n",
      "SVM                    0.592728  0.111594  0.036535\n",
      "\n",
      "Correlated features: 57\n",
      "Removed 30 features:\n",
      " ['VCF0107_oh5', 'VCF0108', 'VCF0112_oh2', 'VCF0127', 'VCF0450', 'VCF9009', 'VCF0742', 'VCF0905', 'VCF9030_oh5', 'VCF0207_int', 'VCF0219_int', 'VCF0224', 'VCF0425_dk', 'VCF0624', 'VCF0114_r2', 'VCF0504_dk', 'VCF0513_dk', 'VCF0541_dk', 'VCF0549_dk', 'VCF0804_dk', 'VCF0803_dk', 'VCF0804', 'VCF0110', 'VCF0303', 'VCF0301_int', 'VCF0870', 'VCF0714_oh1', 'VCF9131_oh2', 'VCF9132_oh1', 'VCF9133_oh2']\n",
      "Operations: ['add thermometer intensity', 'add ordinal intensity', 'add dk sum', 'drop first one-hot', 'break correlation']\n",
      "Number of features selected: 116\n",
      "                           mean     range       std\n",
      "Logistic Regression    0.666641  0.078254  0.028646\n",
      "Bernoulli Naive Bayes  0.649406  0.074603  0.025248\n",
      "AdaBoost               0.639110  0.041074  0.014747\n",
      "SVM                    0.595329  0.099552  0.033006\n",
      "\n",
      "best f1 for Logistic Regression is 0.6757557896073166 with combination: ['add thermometer intensity', 'break correlation']\n",
      "best f1 for AdaBoost is 0.6627276216593307 with combination: ['add thermometer intensity', 'add ordinal intensity', 'add dk sum', 'break correlation']\n",
      "best f1 for Bernoulli Naive Bayes is 0.6514661712180857 with combination: ['add dk sum', 'drop first one-hot']\n",
      "best f1 for SVM is 0.6003801409093159 with combination: ['add thermometer intensity', 'add dk sum', 'drop first one-hot', 'break correlation']\n"
     ]
    }
   ],
   "source": [
    "max_f1 = [0,0,0,0]\n",
    "best_config = [0,0,0,0]\n",
    "X_train_features = [0,0,0,0]\n",
    "\n",
    "classifiers = [LogisticRegression(), AdaBoostClassifier(), BernoulliNB(), SVC()]\n",
    "clf_names = ['Logistic Regression', 'AdaBoost', 'Bernoulli Naive Bayes', 'SVM']\n",
    "\n",
    "for i in range(0, 32):\n",
    "    X_train = X_train_orig\n",
    "    operations = []\n",
    "    flags = '{:05d}'.format(int(bin(i)[2:]))\n",
    "    if int(flags[0]):\n",
    "        X_train = add_thermometer_intensity(X_train)\n",
    "        operations.append('add thermometer intensity')\n",
    "    if int(flags[1]):\n",
    "        X_train = add_ordinal_intensity(X_train)\n",
    "        operations.append('add ordinal intensity')\n",
    "    if int(flags[2]):\n",
    "        X_train = add_dk_sum(X_train)\n",
    "        operations.append('add dk sum')\n",
    "    if int(flags[3]):        \n",
    "        X_train = drop_first_onehot(X_train)\n",
    "        operations.append('drop first one-hot')\n",
    "    if int(flags[4]):\n",
    "        X_train = break_correlation(X_train, y_train)\n",
    "        operations.append('break correlation')\n",
    "    columns = X_train.columns\n",
    "    X_train, rfecv = feature_elimination(X_train, y_train, preprocessing, LogisticRegression())\n",
    "    print('Operations:', operations)\n",
    "    print('Number of features selected:', rfecv.n_features_)\n",
    "    scores = cv_test(X_train, y_train, None, classifiers, clf_names, cv = 5)\n",
    "    print(scores.sort_values(by = 'mean', ascending = False))\n",
    "    \n",
    "    selected_features = columns[rfecv.ranking_ == 1]\n",
    "    for i in range(0,4):\n",
    "        if max_f1[i] < scores['mean'][i]:\n",
    "            max_f1[i] = scores['mean'][i]\n",
    "            best_config[i] = operations\n",
    "            X_train_features[i] = selected_features\n",
    "    print('')\n",
    "\n",
    "for f1, config, name in zip(max_f1, best_config, clf_names):\n",
    "    print('best f1 for {} is {} with combination: {}'.format(name, f1, config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show the best F1 scores achievable for each classification algorithm using combinations of feature modifications in concert with RFE. Because the standard deviations of the k-fold F1 scores are slightly high for the differences in mean F1 scores observed among different feature combinations, we perform a final test as a sanity check using 10-fold cross validation and the recommended feature combinations for each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         mean     range      std\n",
      "Logistic Regression  0.674409  0.100275  0.02989 \n",
      "\n",
      "              mean     range      std\n",
      "AdaBoost  0.658064  0.089444  0.02627 \n",
      "\n",
      "                           mean     range       std\n",
      "Bernoulli Naive Bayes  0.651935  0.129543  0.033571 \n",
      "\n",
      "         mean     range       std\n",
      "SVM  0.595607  0.135197  0.036621 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_all = add_dk_sum(add_thermometer_intensity(add_ordinal_intensity(X_train_orig)))\n",
    "X_train_config = [0,0,0,0]\n",
    "for i in range(0, len(X_train_config)):\n",
    "    X_train_config[i] = X_train_all.loc[:,X_train_features[i]]\n",
    "\n",
    "print(cv_test(X_train_config[0], y_train, preprocessing, [LogisticRegression()], ['Logistic Regression'], cv = 10),'\\n')\n",
    "print(cv_test(X_train_config[1], y_train, preprocessing, [AdaBoostClassifier()], ['AdaBoost'], cv = 10),'\\n')\n",
    "print(cv_test(X_train_config[2], y_train, preprocessing, [BernoulliNB()], ['Bernoulli Naive Bayes'], cv = 10),'\\n')\n",
    "print(cv_test(X_train_config[3], y_train, preprocessing, [SVC()], ['SVM'], cv = 10),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results are consistent with what we found in the original tests. The 10-fold F1 score for logistic regression, while lower than that of the highest 5-fold test, remains a higher score than all but one of any of the scores for other feature combinations. The same is true for Adaboost and Bernoulli Naive Bayes, which have higher 10-fold F1 scores than the F1 scores for any other feature combination. SVM's perform less well, but they are also the least stable, as their 5-fold scores consistently showed higher standard deviations than the rest. \n",
    "\n",
    "We output CSV's for each modified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3886, 123)\n",
      "(3886, 90)\n",
      "(3886, 84)\n",
      "(3886, 141)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(X_train_config)):\n",
    "    print(X_train_config[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "X_train_config[0].to_csv('../data/anes_cdf_training_lr.csv')\n",
    "X_train_config[1].to_csv('../data/anes_cdf_training_ada.csv')\n",
    "X_train_config[2].to_csv('../data/anes_cdf_training_bnb.csv')\n",
    "X_train_config[3].to_csv('../data/anes_cdf_training_svm.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
